# Signal Storage and Usage

**Date:** 2025-01-15  
**Status:** ✅ **DOCUMENTED**

## Overview

This document explains how signals are stored and used in the Argo trading system.

## Signal Storage

### 1. Database Storage (SQLite)

Signals are stored in a SQLite database using the `SignalTracker` class.

#### Database Schema

```sql
CREATE TABLE signals (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    signal_id TEXT UNIQUE NOT NULL,
    symbol TEXT NOT NULL,
    action TEXT NOT NULL,
    entry_price REAL NOT NULL,
    target_price REAL NOT NULL,
    stop_price REAL NOT NULL,
    confidence REAL NOT NULL,
    strategy TEXT NOT NULL,
    asset_type TEXT NOT NULL,
    data_source TEXT DEFAULT 'weighted_consensus',
    timestamp TEXT NOT NULL,
    outcome TEXT DEFAULT NULL,
    exit_price REAL DEFAULT NULL,
    profit_loss_pct REAL DEFAULT NULL,
    sha256 TEXT NOT NULL,
    order_id TEXT DEFAULT NULL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
)
```

#### Storage Location

- **Database File:** `data/signals.db` (SQLite database)
- **Log File:** `data/signals.log` (text log for audit trail)

#### Storage Process

1. **Signal Generation**
   - Signals are generated by `SignalGenerationService`
   - Each signal gets a unique `signal_id` (SHA-256 hash, first 16 chars)
   - Signal data includes: symbol, action, prices, confidence, etc.

2. **Batch Insertion (Optimized)**
   - Signals are queued in `_pending_signals` list
   - Batch size: 50 signals (configurable)
   - Batch timeout: 5 seconds
   - Uses async batch inserts for non-blocking storage

3. **SHA-256 Verification**
   - Each signal is hashed using SHA-256
   - Hash includes: signal_id, symbol, action, entry_price, target_price, stop_price, confidence, strategy, timestamp
   - Ensures data integrity

#### Storage Methods

**Synchronous:**
```python
signal_id = tracker.log_signal(signal)
```

**Asynchronous (Preferred):**
```python
signal_id = await tracker.log_signal_async(signal)
```

### 2. File Logging

Signals are also logged to a text file for audit trail:
- **File:** `data/signals.log`
- **Format:** JSON lines (one signal per line)
- **Purpose:** Audit trail, debugging, backup

### 3. Alpine Backend Sync

Signals are synced to the Alpine backend via HTTP API:
- **Service:** `AlpineSyncService`
- **Endpoint:** `http://91.98.153.49:8001/api/signals`
- **Method:** Async, non-blocking
- **Retry Logic:** Up to 3 retries with exponential backoff

## Signal Usage

### 1. Signal Generation Flow

```
1. Generate signal for symbol
   ↓
2. Validate signal (confidence threshold, risk checks)
   ↓
3. Store signal in database (async batch insert)
   ↓
4. Sync to Alpine backend (async)
   ↓
5. Track signal generation (lifecycle tracker)
   ↓
6. Execute trade (if auto_execute enabled)
   ↓
7. Track trade execution
```

### 2. Signal Processing

**Location:** `_process_and_store_signal()` in `signal_generation_service.py`

**Steps:**
1. **Store Signal**
   ```python
   signal_id = await self.tracker.log_signal_async(signal)
   signal["signal_id"] = signal_id
   ```

2. **Sync to Alpine**
   ```python
   self._sync_signal_to_alpine(signal)
   ```

3. **Track Generation**
   ```python
   self._track_signal_generated(signal, signal_id, symbol)
   ```

4. **Execute Trade** (if enabled)
   ```python
   if self.auto_execute and self.trading_engine:
       executed = await self._execute_trade_if_valid(...)
       self._track_trade_execution(signal, signal_id, executed)
   ```

### 3. Signal Tracking

Signals are tracked through their lifecycle:

- **Signal Generated:** Recorded in lifecycle tracker
- **Trade Executed:** Linked to order_id
- **Trade Skipped:** Reason logged
- **Outcome Tracking:** Exit price, P&L tracked when position closes

### 4. Signal Retrieval

Signals can be retrieved from the database for:
- **Backtesting:** Historical signal analysis
- **Performance Analysis:** Signal quality metrics
- **Debugging:** Signal generation issues
- **Reporting:** Signal statistics

## Performance Optimizations

### 1. Batch Inserts
- Signals are batched (50 per batch)
- Reduces database write overhead
- Improves throughput

### 2. Async Operations
- Database writes are async (non-blocking)
- Alpine sync is async (non-blocking)
- Doesn't block signal generation pipeline

### 3. Connection Pooling
- SQLite connection pool (max 5 connections)
- Reuses connections for better performance
- WAL mode for better concurrency

### 4. Indexes
- Database indexes on frequently queried columns
- Faster signal retrieval
- Optimized query performance

## Signal Data Structure

```python
signal = {
    "signal_id": "abc123...",  # Unique identifier
    "symbol": "AAPL",
    "action": "BUY",  # or "SELL"
    "entry_price": 150.50,
    "target_price": 158.00,
    "stop_price": 146.00,
    "confidence": 85.5,  # 0-100
    "strategy": "weighted_consensus",
    "asset_type": "stock",
    "data_source": "weighted_consensus",
    "timestamp": "2025-01-15T10:30:00Z",
    "regime": "BULL",  # Market regime
    "reasoning": "AI-generated explanation",
    "sources": 5,  # Number of data sources
    "agreement": 0.85  # Source agreement level
}
```

## Signal Lifecycle

1. **Generated:** Signal created by consensus engine
2. **Stored:** Saved to database and file log
3. **Synced:** Sent to Alpine backend
4. **Tracked:** Recorded in lifecycle tracker
5. **Executed:** Trade placed (if auto_execute enabled)
6. **Monitored:** Position tracked until close
7. **Outcome:** Exit price and P&L recorded

## Configuration

### Signal Storage Settings

- **Batch Size:** 50 signals (configurable)
- **Batch Timeout:** 5 seconds
- **Database:** SQLite with WAL mode
- **Connection Pool:** Max 5 connections
- **Indexes:** Enabled for performance

### Signal Execution Settings

- **Auto Execute:** Controlled by `auto_execute` config
- **Confidence Threshold:** Minimum confidence required
- **Risk Checks:** Position limits, drawdown limits
- **Prop Firm Rules:** Additional restrictions if enabled

## Monitoring

### Signal Metrics

- **Generation Rate:** Signals per second
- **Storage Latency:** Time to store signal
- **Sync Success Rate:** Alpine backend sync success
- **Execution Rate:** Signals executed vs skipped
- **Outcome Tracking:** Win rate, average P&L

### Logging

- **Signal Generation:** INFO level
- **Storage:** DEBUG level
- **Sync:** DEBUG level
- **Execution:** INFO level
- **Errors:** ERROR level

## Best Practices

1. **Always use async storage** for non-blocking operations
2. **Batch signals** for better database performance
3. **Monitor storage latency** to ensure <500ms target
4. **Track signal outcomes** for quality analysis
5. **Sync to Alpine** for centralized monitoring
6. **Use connection pooling** for database efficiency

## Troubleshooting

### Signals Not Storing

- Check database file permissions
- Verify connection pool not exhausted
- Check batch flush is working
- Review error logs

### Slow Storage

- Check batch size (should be 50)
- Verify async operations enabled
- Check database indexes exist
- Monitor connection pool usage

### Alpine Sync Failing

- Check network connectivity
- Verify Alpine backend is running
- Check retry logic is working
- Review sync error logs

---

**Status:** ✅ **SIGNAL STORAGE AND USAGE DOCUMENTED**

