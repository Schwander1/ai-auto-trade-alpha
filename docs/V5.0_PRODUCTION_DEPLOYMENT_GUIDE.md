# v5.0 Production Deployment Guide

**Date:** January 15, 2025  
**Status:** ✅ Ready for Production Deployment

---

## Pre-Deployment Checklist

### ✅ Completed

- [x] All v5.0 optimizations implemented
- [x] Dependencies installed (pyarrow, scikit-learn, websockets, boto3)
- [x] Core system health checks passing (6/6)
- [x] Confidence calibrator working
- [x] Outcome tracker working
- [x] Signal generation integration complete
- [x] Parquet support available
- [x] ML support available

### ⚠️ Production Configuration Required

- [ ] AWS credentials configured (for S3 backups)
- [ ] S3 bucket name set (BACKUP_BUCKET_NAME or AWS_BUCKET_NAME)
- [ ] S3 lifecycle policy created
- [ ] Production environment variables set
- [ ] Redis configured (optional, for distributed caching)

---

## Deployment Steps

### Step 1: Install Dependencies

```bash
cd argo
source venv/bin/activate
pip install -r requirements.txt
```

**Verify:**
```bash
python -c "import pandas, pyarrow, sklearn, boto3; print('✅ All dependencies installed')"
```

### Step 2: Configure AWS (Production)

**Set Environment Variables:**
```bash
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"
export AWS_DEFAULT_REGION="us-east-1"
export BACKUP_BUCKET_NAME="your-backup-bucket"
```

**Or use AWS Secrets Manager:**
- Configure in AWS Secrets Manager
- System will auto-detect

### Step 3: Create S3 Lifecycle Policy

```bash
cd argo
source venv/bin/activate
python argo/compliance/s3_lifecycle_policy.py create
```

**Verify:**
```bash
python argo/compliance/s3_lifecycle_policy.py get
```

### Step 4: Test Backup System

```bash
# Test Parquet backup
python argo/compliance/daily_backup.py

# Verify backup was created
# Check S3 bucket for new Parquet file
```

### Step 5: Run Comprehensive Health Check

```bash
# v5.0 optimizations check
python argo/scripts/health_check_v5_optimizations.py

# Full system health check (Level 3)
python argo/scripts/health_check_unified.py --level 3
```

**Expected Results:**
- ✅ All core systems: PASS
- ✅ v5.0 optimizations: 5-7/7 PASS (AWS-dependent may fail in dev)
- ✅ Signal generation: PASS
- ✅ Confidence calibration: PASS
- ✅ Outcome tracking: PASS

### Step 6: Verify Signal Generation

```bash
# Test signal generation with confidence calibration
python -c "
from argo.core.signal_generation_service import SignalGenerationService
import asyncio

async def test():
    service = SignalGenerationService()
    signal = await service.generate_signal_for_symbol('AAPL')
    if signal:
        print(f'✅ Signal generated: {signal[\"symbol\"]} {signal[\"action\"]} @ {signal[\"confidence\"]}%')
        if 'raw_confidence' in signal:
            print(f'   Raw: {signal[\"raw_confidence\"]}% → Calibrated: {signal[\"confidence\"]}%')
    else:
        print('⚠️  No signal generated (may be normal)')

asyncio.run(test())
"
```

### Step 7: Monitor Initial Backups

**First Backup:**
```bash
# Run first backup with Parquet format
python argo/compliance/daily_backup.py

# Verify:
# - Parquet file created
# - File size is ~90% smaller than CSV
# - Verification passed
```

**Monitor:**
- Check backup logs for Parquet format
- Verify compression ratio (~90% reduction)
- Confirm S3 upload successful

---

## Production Configuration

### Environment Variables

**Required for S3 Backups:**
```bash
export BACKUP_BUCKET_NAME="your-backup-bucket-name"
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"
export AWS_DEFAULT_REGION="us-east-1"
```

**Optional:**
```bash
export REDIS_URL="redis://localhost:6379"  # For distributed caching
```

### S3 Lifecycle Policy

**Automatic Transitions:**
- 0-30 days: S3 Standard
- 30-365 days: S3 Standard-IA
- 1-7 years: S3 Glacier
- 7+ years: S3 Glacier Deep Archive

**Create Policy:**
```bash
python argo/compliance/s3_lifecycle_policy.py create
```

### Cron Job for Daily Backups

**Add to crontab:**
```bash
# Daily backup at 2:00 AM UTC
0 2 * * * cd /path/to/argo && source venv/bin/activate && python argo/compliance/daily_backup.py >> logs/backup.log 2>&1
```

---

## Post-Deployment Verification

### Immediate (Within 5 minutes)

1. **Health Check:**
   ```bash
   python argo/scripts/health_check_unified.py --level 3
   ```
   **Expected:** All checks PASS

2. **v5.0 Optimizations:**
   ```bash
   python argo/scripts/health_check_v5_optimizations.py
   ```
   **Expected:** 5-7/7 checks PASS (AWS-dependent may need config)

3. **Signal Generation:**
   - Verify signals are being generated
   - Check confidence calibration is active
   - Confirm outcome tracking is working

### Extended (Within 15 minutes)

1. **Monitor Logs:**
   - Check for errors
   - Verify confidence calibration logs
   - Confirm outcome tracking updates

2. **Test Backup:**
   - Run manual backup
   - Verify Parquet format
   - Check S3 upload

3. **Performance Metrics:**
   - Signal generation time: <0.3s
   - Cache hit rate: >80%
   - API calls: <15 per cycle

---

## Rollback Procedure

If issues occur:

1. **Stop Signal Generation:**
   ```bash
   # Stop the service
   pkill -f signal_generation_service
   ```

2. **Revert Code:**
   ```bash
   git checkout v4.0  # Or previous stable version
   ```

3. **Restore Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify Health:**
   ```bash
   python argo/scripts/health_check_unified.py --level 3
   ```

---

## Monitoring

### Key Metrics to Monitor

1. **Storage:**
   - Backup file sizes (should be ~90% smaller)
   - S3 storage costs (should decrease)
   - Backup success rate

2. **Signal Quality:**
   - Confidence calibration accuracy
   - Win rate improvements
   - Outcome tracking coverage

3. **Performance:**
   - Signal generation time
   - Query performance
   - Cache hit rates

### Alerts

Set up alerts for:
- Backup failures
- Health check failures
- Confidence calibration errors
- Outcome tracking failures

---

## Troubleshooting

### Issue: Backup Manager Fails

**Symptom:** `BACKUP_BUCKET_NAME or AWS_BUCKET_NAME environment variable required`

**Solution:**
```bash
export BACKUP_BUCKET_NAME="your-bucket-name"
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"
```

### Issue: Confidence Calibrator Has No Training Data

**Symptom:** `Insufficient training data (0 samples)`

**Solution:**
- This is normal for new deployments
- Calibrator will use simple calibration until enough data
- After 100+ signals with outcomes, ML calibration will activate

### Issue: S3 Lifecycle Policy Fails

**Symptom:** `NoSuchBucket` or permission errors

**Solution:**
- Verify bucket exists
- Check AWS credentials
- Verify IAM permissions for S3 lifecycle management

---

## Success Criteria

### ✅ Deployment Successful When:

1. **Health Checks:**
   - ✅ All core systems: PASS
   - ✅ v5.0 optimizations: 5+/7 PASS
   - ✅ Signal generation: PASS

2. **Functionality:**
   - ✅ Signals generating with confidence calibration
   - ✅ Outcome tracking working
   - ✅ Backups creating in Parquet format

3. **Performance:**
   - ✅ Signal generation: <0.3s
   - ✅ No performance regressions
   - ✅ All optimizations active

---

## Next Steps After Deployment

1. **Monitor for 24 hours:**
   - Watch for errors
   - Track performance metrics
   - Verify backups

2. **Review Results:**
   - Check confidence calibration accuracy
   - Monitor outcome tracking coverage
   - Measure storage cost savings

3. **Optimize:**
   - Fine-tune calibration model
   - Adjust backup schedule if needed
   - Review S3 lifecycle transitions

---

**Status:** ✅ **Ready for Production Deployment**

**Health:** ✅ **100% System Health**

**Next:** Follow deployment steps above

