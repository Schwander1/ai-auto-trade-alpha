#!/usr/bin/env python3
"""
Comprehensive Backtest for All Optimizations
Tests each optimization individually and combined
Compliance: Rule 15 (Backtesting), Rule 03 (Testing)
"""
import sys
import asyncio
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List
import logging

sys.path.insert(0, str(Path(__file__).parent.parent))

from argo.backtest.enhanced_backtester import EnhancedBacktester
from argo.core.feature_flags import FeatureFlags
from argo.core.signal_generation_service import SignalGenerationService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def run_backtest_suite():
    """Run comprehensive backtest suite"""
    symbols = ["AAPL", "NVDA", "TSLA", "MSFT", "BTC-USD", "ETH-USD"]
    results = {}
    
    # Test configurations
    configs = [
        ("baseline", {}),  # No optimizations
        ("weight_optimization", {"optimized_weights": True}),
        ("regime_weights", {"optimized_weights": True, "regime_based_weights": True}),
        ("confidence_88", {"optimized_weights": True, "confidence_threshold_88": True}),
        ("all_optimizations", {
            "optimized_weights": True,
            "regime_based_weights": True,
            "confidence_threshold_88": True,
            "incremental_confidence": True
        })
    ]
    
    for config_name, flags in configs:
        logger.info(f"\n{'='*60}")
        logger.info(f"Testing: {config_name}")
        logger.info(f"{'='*60}")
        
        # Set feature flags
        feature_flags = FeatureFlags()
        for flag, enabled in flags.items():
            if enabled:
                feature_flags.enable(flag)
            else:
                feature_flags.disable(flag)
        
        # Run backtests
        config_results = []
        for symbol in symbols:
            try:
                backtester = EnhancedBacktester()
                # Note: This is a simplified backtest - full implementation would
                # use SignalGenerationService to generate signals
                metrics = await backtester.run_backtest(symbol)
                if metrics:
                    config_results.append({
                        'symbol': symbol,
                        'win_rate': metrics.win_rate_pct,
                        'total_return': metrics.total_return_pct,
                        'sharpe_ratio': metrics.sharpe_ratio,
                        'max_drawdown': metrics.max_drawdown_pct,
                        'total_trades': metrics.total_trades
                    })
            except Exception as e:
                logger.error(f"Error backtesting {symbol}: {e}")
                config_results.append({
                    'symbol': symbol,
                    'error': str(e)
                })
        
        results[config_name] = config_results
    
    # Save results
    output_file = Path(__file__).parent.parent / "reports" / "optimization_backtest_results.json"
    output_file.parent.mkdir(exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"\nâœ… Results saved to: {output_file}")
    
    # Print summary
    print("\n" + "="*60)
    print("BACKTEST RESULTS SUMMARY")
    print("="*60)
    for config_name, config_results in results.items():
        valid_results = [r for r in config_results if 'error' not in r]
        if valid_results:
            avg_win_rate = sum(r['win_rate'] for r in valid_results) / len(valid_results)
            avg_return = sum(r['total_return'] for r in valid_results) / len(valid_results)
            avg_sharpe = sum(r['sharpe_ratio'] for r in valid_results) / len(valid_results)
            print(f"\n{config_name}:")
            print(f"  Avg Win Rate: {avg_win_rate:.2f}%")
            print(f"  Avg Return: {avg_return:.2f}%")
            print(f"  Avg Sharpe: {avg_sharpe:.2f}")
            print(f"  Symbols Tested: {len(valid_results)}")

if __name__ == "__main__":
    asyncio.run(run_backtest_suite())

