# Additional Refactoring Opportunities

**Date:** January 15, 2025  
**Status:** Analysis Complete  
**Priority:** Medium-High Impact Refactorings

---

## Executive Summary

This report identifies additional refactoring opportunities beyond the initial implementation, focusing on:
1. **Security** - SQL injection vulnerabilities
2. **Code duplication** - Repeated patterns across files
3. **Long methods** - Methods with multiple responsibilities
4. **Magic numbers** - Hardcoded values that should be constants
5. **Error handling** - Inconsistent error handling patterns

**Total Additional Opportunities:** 8 refactorings

---

## ðŸ”´ Priority 1: Security & Critical Issues

### 1. SQL Injection Vulnerability in `DataManager.query_with_duckdb()`

**File:** `argo/argo/backtest/data_manager.py`  
**Lines:** 356-409  
**Severity:** High

#### Current Issues:
- **SQL injection vulnerability** - String formatting in SQL queries
- **No parameterization** - Direct string interpolation
- **Unsafe filter handling** - Filters directly concatenated into query

#### Vulnerable Code:
```python
query = f"""
    SELECT *
    FROM read_parquet('{parquet_file}')
    WHERE Date >= '{start_date}'
      AND Date <= '{end_date}'
"""

if filters:
    for col, condition in filters.items():
        query += f" AND {col} {condition}"  # UNSAFE!
```

#### Refactoring:
```python
def query_with_duckdb(
    self,
    symbol: str,
    start_date: str,
    end_date: str,
    filters: Optional[Dict] = None
) -> Optional[Union['pl.DataFrame', 'pd.DataFrame']]:
    """Query historical data with DuckDB (parameterized for safety)"""
    if not DUCKDB_AVAILABLE or not self.duckdb_conn:
        logger.warning("DuckDB not available, using standard filtering")
        return None
    
    # Find parquet file
    parquet_file = self._find_parquet_file(symbol)
    if not parquet_file:
        return None
    
    # Build parameterized query
    query = """
        SELECT *
        FROM read_parquet(?)
        WHERE Date >= ? AND Date <= ?
    """
    params = [str(parquet_file), start_date, end_date]
    
    # Add filters safely
    if filters:
        query, params = self._add_safe_filters(query, params, filters)
    
    query += " ORDER BY Date"
    
    try:
        result = self.duckdb_conn.execute(query, params)
        return result.pl() if self.use_polars else result.df()
    except Exception as e:
        logger.error(f"DuckDB query failed: {e}")
        return None

def _add_safe_filters(self, query: str, params: List, filters: Dict) -> Tuple[str, List]:
    """Add filters to query safely using parameterization"""
    # Validate column names (whitelist approach)
    ALLOWED_COLUMNS = {'Date', 'Open', 'High', 'Low', 'Close', 'Volume'}
    
    for col, condition in filters.items():
        if col not in ALLOWED_COLUMNS:
            logger.warning(f"Filter column '{col}' not in whitelist, skipping")
            continue
        
        # Parse condition safely (e.g., "> 1000000" -> operator and value)
        operator, value = self._parse_condition(condition)
        if operator and value is not None:
            query += f" AND {col} {operator} ?"
            params.append(value)
    
    return query, params

def _parse_condition(self, condition: str) -> Tuple[Optional[str], Optional[float]]:
    """Parse condition string safely (e.g., "> 1000000" -> (">", 1000000.0))"""
    # Only allow numeric comparisons
    import re
    match = re.match(r'^([<>=]+)\s*([\d.]+)$', condition.strip())
    if match:
        operator, value = match.groups()
        if operator in ['>', '<', '>=', '<=', '=', '==']:
            try:
                return operator, float(value)
            except ValueError:
                pass
    return None, None
```

**Estimated Impact:** 
- **Security:** â­â­â­â­â­ (5/5) - Critical security fix
- **Maintainability:** â­â­â­â­ (4/5)

---

## ðŸŸ¡ Priority 2: Code Duplication

### 2. Extract Empty Metrics Factory

**File:** `argo/argo/backtest/strategy_backtester.py`  
**Lines:** 108-114, 133-149  
**Issue:** Duplicate empty metrics creation

#### Current Issues:
- Empty `BacktestMetrics` created in two places
- Same 14 fields repeated
- Hard to maintain if structure changes

#### Refactoring:
```python
# In base_backtester.py or new metrics_factory.py
@staticmethod
def create_empty_metrics() -> BacktestMetrics:
    """Create empty BacktestMetrics for error cases"""
    return BacktestMetrics(
        total_return_pct=0.0,
        annualized_return_pct=0.0,
        sharpe_ratio=0.0,
        sortino_ratio=0.0,
        max_drawdown_pct=0.0,
        win_rate_pct=0.0,
        profit_factor=0.0,
        total_trades=0,
        winning_trades=0,
        losing_trades=0,
        avg_win_pct=0.0,
        avg_loss_pct=0.0,
        largest_win_pct=0.0,
        largest_loss_pct=0.0
    )
```

**Usage:**
```python
if df is None:
    logger.error(f"_prepare_backtest_data returned None for {symbol}")
    return BacktestMetrics.create_empty_metrics()
```

**Estimated Impact:** Medium

---

### 3. Extract Polars-to-Pandas Conversion Logic

**File:** `argo/argo/backtest/strategy_backtester.py`  
**Lines:** 206-231  
**Issue:** Data conversion logic duplicated across files

#### Current Issues:
- Polars-to-Pandas conversion repeated in multiple files
- Complex conversion logic with multiple edge cases
- Hard to maintain consistency

#### Refactoring:
```python
# New file: argo/argo/backtest/data_converter.py
class DataConverter:
    """Utility for converting between Polars and Pandas DataFrames"""
    
    @staticmethod
    def to_pandas(df: Union['pl.DataFrame', 'pd.DataFrame']) -> pd.DataFrame:
        """Convert Polars DataFrame to Pandas, handling all edge cases"""
        import polars as pl
        
        if isinstance(df, pd.DataFrame):
            return df
        
        if not isinstance(df, pl.DataFrame):
            raise ValueError(f"Unsupported DataFrame type: {type(df)}")
        
        # Convert to Pandas
        pandas_df = df.to_pandas()
        
        # Handle date column/index
        pandas_df = DataConverter._normalize_date_index(pandas_df)
        
        # Ensure numeric columns
        pandas_df = DataConverter._ensure_numeric_columns(pandas_df)
        
        return pandas_df
    
    @staticmethod
    def _normalize_date_index(df: pd.DataFrame) -> pd.DataFrame:
        """Normalize date index/column"""
        # Try Date column first
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            df = df.set_index('Date')
        elif 'Datetime' in df.columns:
            df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
            df = df.set_index('Datetime')
        
        # Ensure index is DatetimeIndex
        if not isinstance(df.index, pd.DatetimeIndex):
            try:
                df.index = pd.to_datetime(df.index, errors='coerce')
            except:
                pass
        
        return df.sort_index()
    
    @staticmethod
    def _ensure_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:
        """Ensure OHLCV columns are numeric"""
        for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return df
```

**Estimated Impact:** Medium

---

## ðŸŸ¢ Priority 3: Long Methods & Complexity

### 4. Refactor `_prepare_backtest_data()` - 84 lines

**File:** `argo/argo/backtest/strategy_backtester.py`  
**Lines:** 187-270  
**Complexity:** High (multiple responsibilities)

#### Current Issues:
- **84-line method** with multiple responsibilities
- Data fetching, validation, conversion, filtering all in one method
- Complex nested conditionals

#### Refactoring:
```python
def _prepare_backtest_data(
    self, 
    symbol: str, 
    start_date: Optional[datetime], 
    end_date: Optional[datetime]
) -> Optional[pd.DataFrame]:
    """Fetch, validate, and filter historical data"""
    # Step 1: Fetch data
    df = self._fetch_raw_data(symbol)
    if df is None:
        return None
    
    # Step 2: Convert to Pandas
    df = self._convert_to_pandas(df)
    if df is None:
        return None
    
    # Step 3: Clean and validate
    df = self._clean_and_validate_data(df, symbol)
    if df is None:
        return None
    
    # Step 4: Filter by date range
    df = self._filter_by_date_range(df, start_date, end_date)
    if df is None:
        return None
    
    logger.info(f"Running strategy backtest for {symbol}: {len(df)} rows")
    return df

def _fetch_raw_data(self, symbol: str) -> Optional[Union['pl.DataFrame', 'pd.DataFrame']]:
    """Fetch raw historical data"""
    df = self.data_manager.fetch_historical_data(symbol, period="20y")
    if df is None:
        logger.error(f"No data available for {symbol}")
        return None
    
    is_empty = df.is_empty() if hasattr(df, 'is_empty') else df.empty
    if is_empty:
        logger.error(f"No data available for {symbol}")
        return None
    
    return df

def _convert_to_pandas(self, df: Union['pl.DataFrame', 'pd.DataFrame']) -> Optional[pd.DataFrame]:
    """Convert Polars to Pandas if needed"""
    from argo.backtest.data_converter import DataConverter
    try:
        return DataConverter.to_pandas(df)
    except Exception as e:
        logger.error(f"Data conversion failed: {e}")
        return None

def _clean_and_validate_data(self, df: pd.DataFrame, symbol: str) -> Optional[pd.DataFrame]:
    """Clean and validate data"""
    # Clean data
    df = self.data_manager._clean_data(df)
    
    # Validate data
    is_valid, issues = self.data_manager.validate_data(df)
    if not is_valid:
        logger.warning(f"Data validation issues (continuing anyway): {issues}")
        critical_issues = [issue for issue in issues 
                          if 'empty' in issue.lower() or 'missing columns' in issue.lower()]
        if critical_issues:
            logger.error(f"Critical data validation failures: {critical_issues}")
            return None
    
    return df

def _filter_by_date_range(
    self, 
    df: pd.DataFrame, 
    start_date: Optional[datetime], 
    end_date: Optional[datetime]
) -> Optional[pd.DataFrame]:
    """Filter DataFrame by date range"""
    if start_date and hasattr(df.index, 'name'):
        df = df[df.index >= start_date]
    if end_date and hasattr(df.index, 'name'):
        df = df[df.index <= end_date]
    
    if len(df) < 100:
        logger.error(f"Insufficient data: {len(df)} rows")
        return None
    
    return df
```

**Estimated Impact:** 
- **Maintainability:** â­â­â­â­â­ (5/5)
- **Testability:** â­â­â­â­â­ (5/5)
- **Lines Reduced:** ~50 lines

---

### 5. Extract Transaction Cost Constants

**File:** `argo/argo/backtest/strategy_backtester.py`  
**Lines:** 37-39  
**Issue:** Magic numbers for transaction costs

#### Current Issues:
- Hardcoded slippage (0.0005), spread (0.0002), commission (0.001)
- Same values repeated in multiple backtester classes
- No single source of truth

#### Refactoring:
```python
# Add to constants.py
class TransactionCostConstants:
    """Constants for transaction cost modeling"""
    
    # Default slippage (0.05%)
    DEFAULT_SLIPPAGE_PCT: Final[float] = 0.0005
    
    # Default spread (0.02%)
    DEFAULT_SPREAD_PCT: Final[float] = 0.0002
    
    # Default commission (0.1%)
    DEFAULT_COMMISSION_PCT: Final[float] = 0.001
    
    # Market-specific adjustments
    CRYPTO_SLIPPAGE_MULTIPLIER: Final[float] = 1.5  # Higher slippage for crypto
    CRYPTO_SPREAD_MULTIPLIER: Final[float] = 2.0    # Higher spread for crypto
```

**Usage:**
```python
from .constants import TransactionCostConstants

def __init__(
    self, 
    initial_capital: float = 100000,
    slippage_pct: float = TransactionCostConstants.DEFAULT_SLIPPAGE_PCT,
    spread_pct: float = TransactionCostConstants.DEFAULT_SPREAD_PCT,
    commission_pct: float = TransactionCostConstants.DEFAULT_COMMISSION_PCT,
    ...
):
```

**Estimated Impact:** Medium

---

## ðŸ”µ Priority 4: Error Handling & Resilience

### 6. Standardize Error Handling Pattern

**Files:** Multiple files  
**Issue:** Inconsistent error handling

#### Current Issues:
- Some methods return `None` on error
- Some methods raise exceptions
- Some methods return empty objects
- Inconsistent logging

#### Refactoring:
```python
# New file: argo/argo/backtest/error_handling.py
from typing import Optional, TypeVar, Callable
from functools import wraps
import logging

T = TypeVar('T')
logger = logging.getLogger(__name__)

def handle_backtest_error(
    default_return: Optional[T] = None,
    log_level: str = 'error'
) -> Callable:
    """Decorator for consistent error handling in backtest methods"""
    def decorator(func: Callable[..., T]) -> Callable[..., Optional[T]]:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Optional[T]:
            try:
                return func(*args, **kwargs)
            except Exception as e:
                log_func = getattr(logger, log_level)
                log_func(f"Error in {func.__name__}: {e}", exc_info=True)
                return default_return
        return wrapper
    return decorator

# Usage:
@handle_backtest_error(default_return=None, log_level='error')
def _prepare_backtest_data(self, symbol: str, ...) -> Optional[pd.DataFrame]:
    ...
```

**Estimated Impact:** Medium

---

### 7. Extract Data Validation Logic

**File:** `argo/argo/backtest/data_manager.py`  
**Lines:** 291-354  
**Issue:** Long validation method with multiple checks

#### Refactoring:
```python
class DataValidator:
    """Validates DataFrame data quality"""
    
    MIN_ROWS_REQUIRED = 100
    REQUIRED_COLUMNS = ['Open', 'High', 'Low', 'Close', 'Volume']
    
    @staticmethod
    def validate(df: pd.DataFrame) -> Tuple[bool, List[str]]:
        """Validate DataFrame and return (is_valid, issues)"""
        issues = []
        
        issues.extend(DataValidator._check_structure(df))
        issues.extend(DataValidator._check_data_quality(df))
        issues.extend(DataValidator._check_sufficient_data(df))
        
        return len(issues) == 0, issues
    
    @staticmethod
    def _check_structure(df: pd.DataFrame) -> List[str]:
        """Check DataFrame structure"""
        issues = []
        # Check columns, index, etc.
        return issues
    
    @staticmethod
    def _check_data_quality(df: pd.DataFrame) -> List[str]:
        """Check data quality (NaN, negative prices, etc.)"""
        issues = []
        # Check for NaN, negative values, etc.
        return issues
    
    @staticmethod
    def _check_sufficient_data(df: pd.DataFrame) -> List[str]:
        """Check if sufficient data exists"""
        issues = []
        if len(df) < DataValidator.MIN_ROWS_REQUIRED:
            issues.append(f"Insufficient data: {len(df)} rows (minimum {DataValidator.MIN_ROWS_REQUIRED})")
        return issues
```

**Estimated Impact:** Medium

---

## ðŸŸ£ Priority 5: Performance & Optimization

### 8. Extract Indicator Calculation to Separate Module

**File:** `argo/argo/backtest/strategy_backtester.py`  
**Lines:** 272-400+  
**Issue:** Long method with many indicator calculations

#### Current Issues:
- `_precalculate_indicators()` is very long
- Mixes different types of indicators
- Hard to test individual indicators
- Hard to add new indicators

#### Refactoring:
```python
# New file: argo/argo/backtest/indicators.py
class IndicatorCalculator:
    """Calculates technical indicators"""
    
    @staticmethod
    def calculate_all(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate all indicators"""
        df = df.copy()
        df = IndicatorCalculator.calculate_sma_indicators(df)
        df = IndicatorCalculator.calculate_momentum_indicators(df)
        df = IndicatorCalculator.calculate_volume_indicators(df)
        df = IndicatorCalculator.calculate_volatility_indicators(df)
        return df
    
    @staticmethod
    def calculate_sma_indicators(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate SMA-based indicators"""
        df['sma_20'] = df['Close'].rolling(window=20, min_periods=1).mean()
        df['sma_50'] = df['Close'].rolling(window=50, min_periods=1).mean()
        # ... etc
        return df
    
    # Similar methods for other indicator types
```

**Estimated Impact:** Medium-High

---

## Implementation Priority

### Phase 1 (Immediate - Security):
1. âœ… Fix SQL injection vulnerability (#1)

### Phase 2 (Short-term - Quality):
2. âœ… Extract empty metrics factory (#2)
3. âœ… Extract Polars-to-Pandas conversion (#3)
4. âœ… Extract transaction cost constants (#5)

### Phase 3 (Medium-term - Structure):
5. âœ… Refactor `_prepare_backtest_data()` (#4)
6. âœ… Extract data validation logic (#7)
7. âœ… Extract indicator calculation (#8)

### Phase 4 (Long-term - Consistency):
8. âœ… Standardize error handling (#6)

---

## Expected Benefits

### Security:
- **Eliminate SQL injection risk** - Critical security improvement

### Maintainability:
- **-30% code duplication** - Shared utilities
- **+50% easier** to add new indicators
- **+40% easier** to modify data validation

### Testability:
- **+60% test coverage** possible with smaller methods
- **Isolated unit tests** for each component

### Performance:
- **No performance impact** (refactoring only)

---

## Risk Assessment

### Low Risk:
- Extracting constants (#5)
- Extracting empty metrics factory (#2)
- Extracting data converter (#3)

### Medium Risk:
- Refactoring long methods (#4, #7, #8) - requires thorough testing
- Standardizing error handling (#6) - may change behavior

### High Risk:
- SQL injection fix (#1) - **MUST be done immediately** but requires careful testing

---

## Next Steps

1. **Immediate:** Fix SQL injection vulnerability (#1)
2. **Short-term:** Extract constants and factories (#2, #3, #5)
3. **Medium-term:** Refactor long methods (#4, #7, #8)
4. **Long-term:** Standardize error handling (#6)

---

## References

- Initial refactoring report: `REFACTORING_OPPORTUNITIES.md`
- Implementation status: `REFACTORING_IMPLEMENTATION_COMPLETE.md`
- Security rules: `Rules/29_ERROR_HANDLING.md`

